{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c3d1e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\Desktop\\college project\\SpeachNonSpeachDeticition\\myenv\\Lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.5.1 when using version 1.7.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Hp\\Desktop\\college project\\SpeachNonSpeachDeticition\\myenv\\Lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.5.1 when using version 1.7.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the trained model\n",
    "model = joblib.load(\"speech_non_speech_rf.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "077b202c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_20720\\4294088006.py:5: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(audio_path, sr=16000)\n",
      "c:\\Users\\Hp\\Desktop\\college project\\SpeachNonSpeachDeticition\\myenv\\Lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "# Load an audio file (mono, 16kHz recommended)\n",
    "audio_path = \"voice.wav\"  # <-- change this to your file\n",
    "y, sr = librosa.load(audio_path, sr=16000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2cd9cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def extract_features(y, sr):\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    zcr = librosa.feature.zero_crossing_rate(y)\n",
    "    centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "    \n",
    "    # Take mean of each feature\n",
    "    features = np.hstack([\n",
    "        np.mean(mfccs, axis=1),\n",
    "        np.mean(zcr),\n",
    "        np.mean(centroid),\n",
    "        np.mean(rolloff)\n",
    "    ])\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "637c7a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected features: 78\n"
     ]
    }
   ],
   "source": [
    "print(\"Expected features:\", model.n_features_in_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05dbf1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features shape: (16,)\n"
     ]
    }
   ],
   "source": [
    "X_test = extract_features(y, sr)\n",
    "print(\"Extracted features shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69a286d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "def extract_features(y, sr):\n",
    "    # --- Core spectral features ---\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=26).T, axis=0)           # 26\n",
    "    chroma = np.mean(librosa.feature.chroma_stft(y=y, sr=sr).T, axis=0)              # 12\n",
    "    mel = np.mean(librosa.feature.melspectrogram(y=y, sr=sr).T, axis=0)[:26]         # 26\n",
    "    contrast = np.mean(librosa.feature.spectral_contrast(y=y, sr=sr).T, axis=0)      # 7\n",
    "    tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr).T, axis=0) # 6\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(y))                             # 1\n",
    "\n",
    "    # --- Combine ---\n",
    "    features = np.hstack([mfccs, chroma, mel, contrast, tonnetz, zcr])\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f363126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features shape: (1, 78)\n",
      "ðŸ”‡ Non-speech detected\n"
     ]
    }
   ],
   "source": [
    "X_test = extract_features(y, sr).reshape(1, -1)\n",
    "print(\"Extracted features shape:\", X_test.shape)\n",
    "\n",
    "prediction = model.predict(X_test)\n",
    "print(\"ðŸŽ¤ Speech detected\" if prediction[0] == 1 else \"ðŸ”‡ Non-speech detected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfacbaaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
